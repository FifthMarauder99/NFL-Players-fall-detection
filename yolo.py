# -*- coding: utf-8 -*-
"""Yolo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FP_TIatrebiTjPAa_qu2l3qwaktA7kqw
"""



import torch
import cv2
import numpy as np
from PIL import Image
import pandas as pd
import matplotlib.pyplot as plt

import mediapipe as mp

mpPose=mp.solutions.pose
pose=mpPose.Pose(min_detection_confidence=0.1, min_tracking_confidence=0.1)
mpDraw=mp.solutions.drawing_utils

# Load YOLOv5 model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

# Open video file
cap = cv2.VideoCapture('test1.mp4')

# Get video dimensions and fps
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

# Create output video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter('output_video.mp4', fourcc, fps, (width, height))

# Process each frame of the video
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    
    # Run detection on frame
    results = model(frame)
    frame1=frame
    # Draw bounding boxes on frame
    frame = results.render()
    frame = np.array(frame[0])
    frame1=np.array(frame1)
    temp_df=results.pandas().xyxy[0]
    temp_df=temp_df.loc[temp_df['class']==0].reset_index()
    #PIL_img=Image.fromarray(frame)
    #PIL_img.save('test.jpg')
    #print(temp_df)
    print("------------------")
   

    for i in range(0,temp_df.shape[0]):
        xmin=int(np.floor(temp_df.loc[i,'ymin']))
        xmax=int(np.ceil(temp_df.loc[i,'ymax']))
        ymin=int(np.floor(temp_df.loc[i,'xmin']))
        ymax=int(np.ceil(temp_df.loc[i,'xmax']))

        # print(frame1.crop((xmin,ymin,xmax,ymax)))
        # print(xmin,xmax,ymin,ymax)
        # print(np.shape(frame))
        # print(frame[10:500+1, 10:100+1, :])
        # print(frame[xmin:xmax+1, ymin:ymax+1 , :])
        # PIL_img=Image.fromarray(frame[xmin:xmax, ymin:ymax,:])
        # PIL_img.save('test1.jpg')
    
#converting image to rgb
        # img_rgb=cv2.cvtColor(frame[xmin:xmax+1, ymin:ymax+1], cv2.COLOR_BGR2RGB)

        results=pose.process(frame1[xmin:xmax, ymin:ymax,:])

        if results.pose_landmarks:
            mpDraw.draw_landmarks(frame1[xmin:xmax, ymin:ymax, :], results.pose_landmarks, mpPose.POSE_CONNECTIONS)
    
    
    # Write output frame to video file
    out.write(frame1)
    
    # Display frame (optional)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release resources
cap.release()
out.release()
cv2.destroyAllWindows()

